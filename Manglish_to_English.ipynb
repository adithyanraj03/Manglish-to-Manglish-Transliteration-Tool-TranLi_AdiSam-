{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T11:22:10.866650Z",
     "start_time": "2024-09-04T11:22:10.860297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch is using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU index: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"GPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.1f} GB\")\n",
    "    print(f\"  Cached:    {torch.cuda.memory_reserved(0) / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU instead.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using device: cpu\n",
      "No GPU available, using CPU instead.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7wtWButH0ZtS",
    "ExecuteTime": {
     "end_time": "2024-09-04T06:46:16.584816Z",
     "start_time": "2024-09-04T06:46:16.581041Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n"
   ],
   "metadata": {
    "id": "YkOzbRSu08uI",
    "ExecuteTime": {
     "end_time": "2024-09-04T06:46:16.615160Z",
     "start_time": "2024-09-04T06:46:16.610827Z"
    }
   },
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n"
   ],
   "metadata": {
    "id": "aw4d9kwa1How",
    "ExecuteTime": {
     "end_time": "2024-09-04T06:46:16.646621Z",
     "start_time": "2024-09-04T06:46:16.641692Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the seq2seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio  # Changed this line\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ],
   "metadata": {
    "id": "-kPnF0Pe1TYJ",
    "ExecuteTime": {
     "end_time": "2024-09-04T06:46:16.654308Z",
     "start_time": "2024-09-04T06:46:16.648631Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom dataset\n",
    "class ManglishEnglishDataset(Dataset):\n",
    "    def __init__(self, manglish_texts, english_texts, manglish_vocab, english_vocab, max_len):\n",
    "        self.manglish_texts = manglish_texts\n",
    "        self.english_texts = english_texts\n",
    "        self.manglish_vocab = manglish_vocab\n",
    "        self.english_vocab = english_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manglish_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        manglish = [self.manglish_vocab['<sos>']] + [self.manglish_vocab.get(word, self.manglish_vocab['<unk>']) for word in self.manglish_texts[idx].split()] + [self.manglish_vocab['<eos>']]\n",
    "        english = [self.english_vocab['<sos>']] + [self.english_vocab.get(word, self.english_vocab['<unk>']) for word in self.english_texts[idx].split()] + [self.english_vocab['<eos>']]\n",
    "\n",
    "        # Pad sequences\n",
    "        manglish = manglish + [self.manglish_vocab['<pad>']] * (self.max_len - len(manglish))\n",
    "        english = english + [self.english_vocab['<pad>']] * (self.max_len - len(english))\n",
    "\n",
    "        return torch.tensor(manglish[:self.max_len]), torch.tensor(english[:self.max_len])\n"
   ],
   "metadata": {
    "id": "er_vUQXK1XIA",
    "ExecuteTime": {
     "end_time": "2024-09-04T06:46:16.673089Z",
     "start_time": "2024-09-04T06:46:16.666317Z"
    }
   },
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to build vocabulary\n",
    "def build_vocab(texts):\n",
    "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    return vocab"
   ],
   "metadata": {
    "id": "pos-IlhO1bnw",
    "ExecuteTime": {
     "end_time": "2024-09-04T06:46:16.692917Z",
     "start_time": "2024-09-04T06:46:16.688098Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "# Training function\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device).transpose(0, 1), trg.to(device).transpose(0, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].contiguous().view(-1, output_dim)\n",
    "        trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ],
   "metadata": {
    "id": "jIlm1ICZ1dyA",
    "ExecuteTime": {
     "end_time": "2024-09-04T06:46:16.730517Z",
     "start_time": "2024-09-04T06:46:16.724934Z"
    }
   },
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to translate\n",
    "def translate_manglish_to_english(model, sentence, manglish_vocab, english_vocab, max_len_manglish):\n",
    "    model.eval()\n",
    "    tokens = [manglish_vocab.get(word, manglish_vocab['<unk>']) for word in sentence.split()]\n",
    "    tokens = tokens + [manglish_vocab['<pad>']] * (max_len_manglish - len(tokens))\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "    trg_tokens = [english_vocab['<sos>']]\n",
    "    for _ in range(50):\n",
    "        trg_tensor = torch.LongTensor([trg_tokens[-1]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "        pred_token = output.argmax(1).item()\n",
    "        if pred_token == english_vocab['<eos>'] or pred_token == english_vocab['<pad>']:\n",
    "            break\n",
    "        trg_tokens.append(pred_token)\n",
    "    trg_tokens = trg_tokens[1:]\n",
    "    return ' '.join([list(english_vocab.keys())[list(english_vocab.values()).index(i)] for i in trg_tokens])\n",
    "\n"
   ],
   "metadata": {
    "id": "0TywoWvD1gPI",
    "ExecuteTime": {
     "end_time": "2024-09-04T06:46:16.755554Z",
     "start_time": "2024-09-04T06:46:16.750040Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcXiBjPR58ko",
    "outputId": "7ba9c78f-d22f-4ad1-c1d7-3077438b7e6e",
    "ExecuteTime": {
     "end_time": "2024-09-04T06:46:19.190338Z",
     "start_time": "2024-09-04T06:46:16.781564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (2.21.0)\n",
      "Requirement already satisfied: filelock in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (0.24.6)\n",
      "Requirement already satisfied: packaging in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in b:\\application\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from datasets import load_dataset\n",
    "    import torch.optim as optim\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # Load a larger dataset (example using Hugging Face datasets)\n",
    "    dataset = load_dataset('opus100', 'en-ml', split='train')\n",
    "\n",
    "    # Preprocess the dataset (this is a simplified example)\n",
    "    manglish_texts = [example['translation']['ml'] for example in dataset]  # Assuming 'ml' is Malayalam\n",
    "    english_texts = [example['translation']['en'] for example in dataset]\n",
    "\n",
    "    # Limit the dataset size for quicker processing (remove this for full dataset)\n",
    "    manglish_texts = manglish_texts[:10000]\n",
    "    english_texts = english_texts[:10000]\n",
    "\n",
    "    # Build vocabularies\n",
    "    manglish_vocab = build_vocab(manglish_texts)\n",
    "    english_vocab = build_vocab(english_texts)\n",
    "\n",
    "    # Calculate a single max length for both Manglish and English\n",
    "    max_len = max(max(len(text.split()) for text in manglish_texts),\n",
    "                  max(len(text.split()) for text in english_texts))\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = ManglishEnglishDataset(manglish_texts, english_texts, manglish_vocab, english_vocab, max_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Model parameters\n",
    "    INPUT_DIM = len(manglish_vocab)\n",
    "    OUTPUT_DIM = len(english_vocab)\n",
    "    ENC_EMB_DIM = 256\n",
    "    DEC_EMB_DIM = 256\n",
    "    HID_DIM = 512\n",
    "    N_LAYERS = 4  # Increased number of layers\n",
    "    ENC_DROPOUT = 0.5\n",
    "    DEC_DROPOUT = 0.5\n",
    "\n",
    "    # Create model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "    model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "    # Training setup\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=manglish_vocab['<pad>'])\n",
    "    N_EPOCHS = 20  # Increased number of epochs\n",
    "    CLIP = 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "        for i, (src, trg) in enumerate(pbar):\n",
    "            src, trg = src.to(device).transpose(0, 1), trg.to(device).transpose(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].contiguous().view(-1, output_dim)\n",
    "            trg = trg[1:].contiguous().view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.3f}'})\n",
    "\n",
    "            # Print intermediate results every 100 batches\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"\\nBatch {i+1}/{len(dataloader)}, Loss: {loss.item():.3f}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f'\\nEpoch: {epoch+1:02} | Average Loss: {avg_loss:.3f}')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    # Test the model\n",
    "    test_sentences = [\n",
    "        \"Njan veetil aanu\",\n",
    "        \"Nale school il pokanam\",\n",
    "        \"Ente peru John aanu\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nTesting the model:\")\n",
    "    for test_sentence in test_sentences:\n",
    "        translation = translate_manglish_to_english(model, test_sentence, manglish_vocab, english_vocab, max_len)\n",
    "        print(f\"Manglish: {test_sentence}\")\n",
    "        print(f\"English: {translation}\")\n",
    "        print()"
   ],
   "metadata": {
    "id": "rweyFP0p1igw",
    "ExecuteTime": {
     "end_time": "2024-09-04T06:46:38.117253Z",
     "start_time": "2024-09-04T06:46:19.192344Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The `scipy` install you are using seems to be broken, (extension modules cannot be imported), please try reinstalling.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[1;32mB:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\scipy\\__init__.py:154\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 154\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_lib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_ccallback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LowLevelCallable\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mB:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\scipy\\_lib\\_ccallback.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _ccallback_c\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mctypes\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name '_ccallback_c' from 'scipy._lib' (B:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\scipy\\_lib\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[44], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Load a larger dataset (example using Hugging Face datasets)\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m dataset \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopus100\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124men-ml\u001B[39m\u001B[38;5;124m'\u001B[39m, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Preprocess the dataset (this is a simplified example)\u001B[39;00m\n\u001B[0;32m     11\u001B[0m manglish_texts \u001B[38;5;241m=\u001B[39m [example[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtranslation\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mml\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m dataset]  \u001B[38;5;66;03m# Assuming 'ml' is Malayalam\u001B[39;00m\n",
      "File \u001B[1;32mB:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\datasets\\load.py:2606\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2601\u001B[0m verification_mode \u001B[38;5;241m=\u001B[39m VerificationMode(\n\u001B[0;32m   2602\u001B[0m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS\n\u001B[0;32m   2603\u001B[0m )\n\u001B[0;32m   2605\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[1;32m-> 2606\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m load_dataset_builder(\n\u001B[0;32m   2607\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m   2608\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2609\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   2610\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   2611\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2612\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   2613\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   2614\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   2615\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   2616\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2617\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   2618\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   2619\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39mname \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2620\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[0;32m   2621\u001B[0m )\n\u001B[0;32m   2623\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[0;32m   2624\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[1;32mB:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\datasets\\load.py:2314\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001B[0m\n\u001B[0;32m   2312\u001B[0m builder_cls \u001B[38;5;241m=\u001B[39m get_dataset_builder_class(dataset_module, dataset_name\u001B[38;5;241m=\u001B[39mdataset_name)\n\u001B[0;32m   2313\u001B[0m \u001B[38;5;66;03m# Instantiate the dataset builder\u001B[39;00m\n\u001B[1;32m-> 2314\u001B[0m builder_instance: DatasetBuilder \u001B[38;5;241m=\u001B[39m builder_cls(\n\u001B[0;32m   2315\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2316\u001B[0m     dataset_name\u001B[38;5;241m=\u001B[39mdataset_name,\n\u001B[0;32m   2317\u001B[0m     config_name\u001B[38;5;241m=\u001B[39mconfig_name,\n\u001B[0;32m   2318\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   2319\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   2320\u001B[0m     \u001B[38;5;28mhash\u001B[39m\u001B[38;5;241m=\u001B[39mdataset_module\u001B[38;5;241m.\u001B[39mhash,\n\u001B[0;32m   2321\u001B[0m     info\u001B[38;5;241m=\u001B[39minfo,\n\u001B[0;32m   2322\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   2323\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2324\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   2325\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbuilder_kwargs,\n\u001B[0;32m   2326\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[0;32m   2327\u001B[0m )\n\u001B[0;32m   2328\u001B[0m builder_instance\u001B[38;5;241m.\u001B[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001B[0;32m   2330\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\n",
      "File \u001B[1;32mB:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\datasets\\builder.py:447\u001B[0m, in \u001B[0;36mDatasetBuilder.__init__\u001B[1;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001B[0m\n\u001B[0;32m    444\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file_format \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    446\u001B[0m \u001B[38;5;66;03m# Enable streaming (e.g. it patches \"open\" to work with remote files)\u001B[39;00m\n\u001B[1;32m--> 447\u001B[0m extend_dataset_builder_for_streaming(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[1;32mB:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\datasets\\streaming.py:120\u001B[0m, in \u001B[0;36mextend_dataset_builder_for_streaming\u001B[1;34m(builder)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;66;03m# this extends the open and os.path.join functions for data streaming\u001B[39;00m\n\u001B[0;32m    119\u001B[0m download_config \u001B[38;5;241m=\u001B[39m DownloadConfig(storage_options\u001B[38;5;241m=\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mstorage_options, token\u001B[38;5;241m=\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mtoken)\n\u001B[1;32m--> 120\u001B[0m extend_module_for_streaming(builder\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m, download_config\u001B[38;5;241m=\u001B[39mdownload_config)\n\u001B[0;32m    121\u001B[0m \u001B[38;5;66;03m# if needed, we also have to extend additional internal imports (like wmt14 -> wmt_utils)\u001B[39;00m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m builder\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatasets.\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# check that it's not a packaged builder like csv\u001B[39;00m\n",
      "File \u001B[1;32mB:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\datasets\\streaming.py:103\u001B[0m, in \u001B[0;36mextend_module_for_streaming\u001B[1;34m(module_path, download_config)\u001B[0m\n\u001B[0;32m    101\u001B[0m patch_submodule(module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpandas.read_csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, wrap_auth(xpandas_read_csv), attrs\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__version__\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m    102\u001B[0m patch_submodule(module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpandas.read_excel\u001B[39m\u001B[38;5;124m\"\u001B[39m, wrap_auth(xpandas_read_excel), attrs\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__version__\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m--> 103\u001B[0m patch_submodule(module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscipy.io.loadmat\u001B[39m\u001B[38;5;124m\"\u001B[39m, wrap_auth(xsio_loadmat), attrs\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__version__\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m    104\u001B[0m patch_submodule(module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxml.etree.ElementTree.parse\u001B[39m\u001B[38;5;124m\"\u001B[39m, wrap_auth(xet_parse))\u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m    105\u001B[0m patch_submodule(module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxml.dom.minidom.parse\u001B[39m\u001B[38;5;124m\"\u001B[39m, wrap_auth(xxml_dom_minidom_parse))\u001B[38;5;241m.\u001B[39mstart()\n",
      "File \u001B[1;32mB:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\datasets\\utils\\patching.py:108\u001B[0m, in \u001B[0;36mpatch_submodule.start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    107\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Activate a patch.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 108\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_active_patches\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[1;32mB:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\datasets\\utils\\patching.py:57\u001B[0m, in \u001B[0;36mpatch_submodule.__enter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(submodules)):\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 57\u001B[0m         submodule \u001B[38;5;241m=\u001B[39m import_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(submodules[: i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m]))\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n\u001B[0;32m     59\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[1;32mB:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\importlib\\__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m     88\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap\u001B[38;5;241m.\u001B[39m_gcd_import(name[level:], package, level)\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1387\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1331\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:935\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:995\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32mB:\\Application\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\scipy\\__init__.py:159\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    156\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe `scipy` install you are using seems to be broken, \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n\u001B[0;32m    157\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(extension modules cannot be imported), \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n\u001B[0;32m    158\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplease try reinstalling.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 159\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_lib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_testutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PytestTester\n\u001B[0;32m    162\u001B[0m test \u001B[38;5;241m=\u001B[39m PytestTester(\u001B[38;5;18m__name__\u001B[39m)\n",
      "\u001B[1;31mImportError\u001B[0m: The `scipy` install you are using seems to be broken, (extension modules cannot be imported), please try reinstalling."
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to get user input\n",
    "def get_user_input():\n",
    "    return input(\"Enter Manglish text to translate: \")\n",
    "\n",
    "# Function to translate user input\n",
    "def translate_user_input(model, manglish_vocab, english_vocab, max_len):\n",
    "    user_input = get_user_input()\n",
    "    translation = translate_manglish_to_english(model, user_input, manglish_vocab, english_vocab, max_len)\n",
    "    return user_input, translation\n",
    "\n",
    "# Function to display the translation\n",
    "def display_translation(original, translation):\n",
    "    print(f\"Original Manglish: {original}\")\n",
    "    print(f\"Translated English: {translation}\")\n",
    "\n",
    "# Main loop for user interaction\n",
    "print(\"Manglish to English Translator\")\n",
    "print(\"Type 'quit' to exit\")\n",
    "\n",
    "while True:\n",
    "    original, translation = translate_user_input(model, manglish_vocab, english_vocab, max_len)\n",
    "    if original.lower() == 'quit':\n",
    "        print(\"Exiting translator. Goodbye!\")\n",
    "        break\n",
    "    display_translation(original, translation)\n",
    "    print()  # Add a blank line for readability"
   ],
   "metadata": {
    "id": "ChwpLMEl1jQA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "K7hfHxei41rJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
